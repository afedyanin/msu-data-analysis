{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеки pandas и numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем набор из двух примеров\n",
    "toy_corpus = ['Машина едет по дороге', \n",
    "              'Грузовик едет по шоссе']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# создаем экземпляр класса CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(toy_corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 6\n",
      "Содержимое словаря:\n",
      " {'машина': 3, 'едет': 2, 'по': 4, 'дороге': 1, 'грузовик': 0, 'шоссе': 5}\n"
     ]
    }
   ],
   "source": [
    "# смотрим размер и содержимое словаря\n",
    "print(\"Размер словаря: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Содержимое словаря:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество признаков: 6\n",
      "Признаки: ['грузовик' 'дороге' 'едет' 'машина' 'по' 'шоссе']\n"
     ]
    }
   ],
   "source": [
    "# вычислим количество признаков \n",
    "# и посмотрим список признаков\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Количество признаков: {}\".format(len(feature_names)))\n",
    "print(\"Признаки:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x6 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 8 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# получаем представление \"мешок слов\"\n",
    "bag_of_words = vect.transform(toy_corpus)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Плотное представление bag_of_words:\n",
      "[[0 1 1 1 1 0]\n",
      " [1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# получаем плотное представление \"мешок слов\"\n",
    "print(\"Плотное представление bag_of_words:\\n{}\".format(\n",
    "    bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое словаря:\n",
      " {'едет': 0, 'по': 1}\n"
     ]
    }
   ],
   "source": [
    "# создаем экземпляр класса CountVectorizer, будем \n",
    "# извлекать слова, которые встретились не менее, \n",
    "# чем в двух документах\n",
    "vect = CountVectorizer(min_df=2)\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(toy_corpus)\n",
    "# смотрим содержимое словаря\n",
    "print(\"Содержимое словаря:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое словаря:\n",
      " {'машина': 2, 'дороге': 1, 'грузовик': 0, 'шоссе': 3}\n"
     ]
    }
   ],
   "source": [
    "# создаем экземпляр класса CountVectorizer, будем \n",
    "# извлекать слова, которые встретились не более, \n",
    "# чем в одном документе \n",
    "vect = CountVectorizer(max_df=1)\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(toy_corpus)\n",
    "# смотрим содержимое словаря\n",
    "print(\"Содержимое словаря:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое словаря:\n",
      " {'машина': 3, 'едет': 2, 'дороге': 1, 'грузовик': 0, 'шоссе': 4}\n"
     ]
    }
   ],
   "source": [
    "# создаем экземпляр класса CountVectorizer, \n",
    "# будем извлекать слова, кроме предлога по\n",
    "vect = CountVectorizer(stop_words=['по'])\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(toy_corpus)\n",
    "# смотрим содержимое словаря\n",
    "print(\"Содержимое словаря:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое словаря:\n",
      " {'машина': 1, 'дороге': 0, 'на': 2}\n"
     ]
    }
   ],
   "source": [
    "# создаем набор из трех примеров\n",
    "toy_corpus = ['Машина едет по дороге', \n",
    "              'Машина столкнулась на дороге с автобусом', \n",
    "              'Машина сбила на дороге пешехода']\n",
    "# создаем экземпляр класса CountVectorizer, будем\n",
    "# извлекать три самых часто встречающихся слова\n",
    "vect = CountVectorizer(max_features=3)\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(toy_corpus)\n",
    "# смотрим содержимое словаря\n",
    "print(\"Содержимое словаря:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаки: ['грузовик' 'грузовик мчится' 'грузовик мчится по' 'грязный'\n",
      " 'грязный грузовик' 'грязный грузовик мчится' 'дороге' 'едет' 'едет по'\n",
      " 'едет по разбитой' 'машина' 'машина едет' 'машина едет по' 'мчится'\n",
      " 'мчится по' 'мчится по широкому' 'по' 'по разбитой' 'по разбитой дороге'\n",
      " 'по широкому' 'по широкому шоссе' 'разбитой' 'разбитой дороге'\n",
      " 'серебристая' 'серебристая машина' 'серебристая машина едет' 'широкому'\n",
      " 'широкому шоссе' 'шоссе']\n"
     ]
    }
   ],
   "source": [
    "# создаем новый набор\n",
    "toy_corpus = ['Серебристая машина едет по разбитой дороге', \n",
    "              'Грязный грузовик мчится по широкому шоссе']\n",
    "# создаем экземпляр класса CountVectorizer, будем извлекать\n",
    "# униграммы, биграммы и триграммы\n",
    "vect = CountVectorizer(ngram_range=(1, 3))\n",
    "vect.fit(toy_corpus)\n",
    "print(\"Признаки:\", vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем данные\n",
    "data = pd.read_csv('Data/Restaurant_Reviews.tsv', \n",
    "                   sep='\\t', \n",
    "                   quoting=3)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['Review'],\n",
    "    data['Liked'], \n",
    "    test_size=0.3,\n",
    "    stratify=data['Liked'],\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество признаков: 1640\n",
      "Первые 100 признаков:\n",
      "['00' '10' '100' '11' '12' '15' '17' '1979' '20' '2007' '23' '30' '35'\n",
      " '40' '45' '4ths' '5lb' '70' '99' 'about' 'above' 'absolute' 'absolutely'\n",
      " 'absolutley' 'accordingly' 'accountant' 'ache' 'acknowledged' 'across'\n",
      " 'actual' 'actually' 'added' 'affordable' 'after' 'afternoon' 'again'\n",
      " 'ago' 'airline' 'airport' 'ala' 'albondigas' 'all' 'allergy' 'almost'\n",
      " 'alone' 'also' 'although' 'always' 'am' 'amazing' 'ambiance' 'ambience'\n",
      " 'amount' 'an' 'and' 'andddd' 'angry' 'annoying' 'another' 'anticipated'\n",
      " 'any' 'anymore' 'anyone' 'anytime' 'anyways' 'apart' 'apology' 'app'\n",
      " 'appalling' 'apparently' 'appealing' 'appetizer' 'apple' 'are' 'area'\n",
      " 'aren' 'aria' 'around' 'array' 'arrived' 'article' 'as' 'ask' 'asked'\n",
      " 'asking' 'assure' 'at' 'ate' 'atmosphere' 'atrocious' 'attack'\n",
      " 'attention' 'attentive' 'attitudes' 'authentic' 'average' 'avocado'\n",
      " 'avoid' 'avoided' 'away']\n"
     ]
    }
   ],
   "source": [
    "# снова создаем экземпляр класса CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "# обучаем CountVectorizer\n",
    "vect.fit(X_train)\n",
    "# вычислим количество признаков и посмотрим \n",
    "# список первых 100 признаков\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Количество признаков: {}\".format(\n",
    "    len(feature_names)))\n",
    "print(\"Первые 100 признаков:\\n{}\".format(\n",
    "    feature_names[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров:\n",
      "{'logreg__C': 100, 'vectorizer__ngram_range': (1, 1)}\n",
      "Наилучшее значение правильности: 0.796\n",
      "Значение правильности на тестовой выборке: 0.803\n"
     ]
    }
   ],
   "source": [
    "# задаем список стоп-слов\n",
    "stop_wrds = ['be', 'is', 'are', 'the', 'a',\n",
    "             'an', 'on', 'of', 'and', 'in']\n",
    "\n",
    "# задаем конвейер\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words=stop_wrds)), \n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# задаем сетку гиперпараметров\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'logreg__C': [.01, .1, .5, 1, 5, 10, 100, 150]\n",
    "}\n",
    "\n",
    "# создаем экземпляр класса GridSearchCV, передав конвейер,\n",
    "# сетку гиперпараметров и указав количество\n",
    "# блоков перекрестной проверки\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid, \n",
    "                  cv=10)\n",
    "# выполняем поиск по всем значениям сетки\n",
    "gs.fit(X_train, y_train)\n",
    "# смотрим наилучшие значения гиперпараметров\n",
    "print(\"Наилучшие значения гиперпараметров:\\n{}\".format(\n",
    "    gs.best_params_))\n",
    "# смотрим наилучшее значение правильности\n",
    "print(\"Наилучшее значение правильности: {:.3f}\".format(\n",
    "    gs.best_score_))\n",
    "# смотрим значение правильности на тестовой выборке\n",
    "print(\"Значение правильности на тестовой выборке: {:.3f}\".format(\n",
    "    gs.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70490949, 0.        , 0.50154891, 0.50154891],\n",
       "       [0.        , 0.70490949, 0.50154891, 0.50154891]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импортируем классы TfidfVectorizer и TfidfTransformer\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer, \n",
    "                                             TfidfTransformer)\n",
    "# создаем экземпляр класса TfidfVectorizer\n",
    "tdidfvectorizer = TfidfVectorizer(smooth_idf=True)\n",
    "# создаем игрушечный корпус из двух \n",
    "# документов - предложений\n",
    "toy_corpus = ['Этот автомобиль едет', \n",
    "              'Этот грузовик едет']\n",
    "# выполним масштабирование tf-idf \n",
    "tfidf = tdidfvectorizer.fit_transform(toy_corpus)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров:\n",
      "{'logreg__C': 5, 'vectorizer__max_features': 4000, 'vectorizer__ngram_range': (1, 2)}\n",
      "Наилучшее значение правильности: 0.800\n",
      "Значение правильности на тестовой выборке: 0.807\n"
     ]
    }
   ],
   "source": [
    "# задаем новую сетку гиперпараметров\n",
    "param_grid2 = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vectorizer__max_features': [2000, 2500, 3000, 3500, 4000, \n",
    "                                 4500, 5000, 6000, 7000],\n",
    "    'logreg__C': [.01, .1, .5, 1, 5, 10, \n",
    "                  100, 150, 200, 300]\n",
    "}\n",
    "\n",
    "# задаем конвейер\n",
    "pipe2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words=stop_wrds)), \n",
    "    ('tfidftransformer', TfidfTransformer()),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# создаем экземпляр класса GridSearchCV\n",
    "gs2 = GridSearchCV(pipe2, \n",
    "                   param_grid2, \n",
    "                   cv=10, \n",
    "                   return_train_score=False)\n",
    "# выполняем поиск по всем значениям сетки\n",
    "gs2.fit(X_train, y_train)\n",
    "\n",
    "# смотрим наилучшие значения гиперпараметров\n",
    "print(\"Наилучшие значения гиперпараметров:\\n{}\".format(\n",
    "    gs2.best_params_))\n",
    "# смотрим наилучшее значение правильности\n",
    "print(\"Наилучшее значение правильности: {:.3f}\".format(\n",
    "    gs2.best_score_))\n",
    "# смотрим значение правильности на тестовой выборке\n",
    "print(\"Значение правильности на тестовой выборке: {:.3f}\".format(\n",
    "    gs2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеку googletrans\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция обратного перевода текста\n",
    "def back_translation(text):\n",
    "    ru_text = translator.translate(text, dest='ru', src='en').text\n",
    "    en_text = translator.translate(ru_text, dest='en', src='ru').text\n",
    "    \n",
    "    return en_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполняем обратный перевод для обучающего набора\n",
    "X_train_back = X_train.apply(back_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700,) (700,)\n"
     ]
    }
   ],
   "source": [
    "# смотрим размер исходного обучающего набора\n",
    "# и переведенного обучающего набора\n",
    "print(X_train.shape, X_train_back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284    I would definitely recommend the wings as well...\n",
      "5         Now I am getting angry and I want my damn pho.\n",
      "368          The staff are great, the ambiance is great.\n",
      "670                                        dont go here.\n",
      "413     I can assure you that you won't be disappointed.\n",
      "Name: Review, dtype: object\n",
      "\n",
      "284      I definitely recommend wings, as well as pizza.\n",
      "5                 Now I am angry and want my damned FOR.\n",
      "368    The staff is magnificent, the atmosphere is ma...\n",
      "670                                      Do not go here.\n",
      "413    I can assure you that you will not be disappoi...\n",
      "Name: Review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# смотрим первые 5 текстов исходного\n",
    "# обучающего набора\n",
    "print(X_train.head())\n",
    "print(\"\")\n",
    "# смотрим первые 5 текстов переведенного \n",
    "# обучающего набора\n",
    "print(X_train_back.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем исходный и переведенный обучающие наборы\n",
    "X_train_new = pd.concat([X_train, X_train_back])\n",
    "y_train_new = pd.concat([y_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1349,) (1349,)\n"
     ]
    }
   ],
   "source": [
    "# избавляемся от записей с одинаковым текстом обзора\n",
    "y_train_new = y_train_new[~X_train_new.duplicated()]\n",
    "X_train_new.drop_duplicates(inplace=True)\n",
    "print(y_train_new.shape, X_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The food was terrible.'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем, нет ли в тестовом наборе текстов, \n",
    "# которые есть в новом обучающем наборе\n",
    "np.intersect1d(X_train_new.values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конкатенируем тестовые массив признаков и массив меток\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "# формируем тестовый набор так, чтобы в нем не было текстов,\n",
    "# имеющихся в новом обучающем наборе\n",
    "test_data = test_data[~test_data['Review'].isin(X_train_new)]\n",
    "# формируем новый тестовый массив признаков\n",
    "X_test = test_data['Review']\n",
    "# формируем новый тестовый массив меток\n",
    "y_test = test_data['Liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров:\n",
      "{'logreg__C': 150, 'vectorizer__max_features': 7000, 'vectorizer__ngram_range': (1, 2)}\n",
      "Наилучшее значение правильности: 0.961\n",
      "Значение правильности на тестовой выборке: 0.833\n"
     ]
    }
   ],
   "source": [
    "# выполняем поиск по всем значениям сетки\n",
    "gs2.fit(X_train_new, y_train_new)\n",
    "print(\"Наилучшие значения гиперпараметров:\\n{}\".format(\n",
    "    gs2.best_params_))\n",
    "print(\"Наилучшее значение правильности: {:.3f}\".format(\n",
    "    gs2.best_score_))\n",
    "print(\"Значение правильности на тестовой выборке: {:.3f}\".format(\n",
    "    gs2.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
